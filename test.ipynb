{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, TQDMProgressBar\n",
    "from pytorch_lightning.strategies import ParallelStrategy\n",
    "from pytorch_lightning.utilities.cli import LightningCLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (C:\\Users\\Yegyanathan V\\.cache\\huggingface\\datasets\\rotten_tomatoes_movie_review\\default\\1.0.0\\40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
      "  0%|          | 0/9 [00:00<?, ?ba/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 9/9 [00:04<00:00,  1.81ba/s]\n",
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (C:\\Users\\Yegyanathan V\\.cache\\huggingface\\datasets\\rotten_tomatoes_movie_review\\default\\1.0.0\\40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.98ba/s]\n",
      "Using custom data configuration default\n",
      "Reusing dataset rotten_tomatoes_movie_review (C:\\Users\\Yegyanathan V\\.cache\\huggingface\\datasets\\rotten_tomatoes_movie_review\\default\\1.0.0\\40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.20ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def _tokenize(x):\n",
    "\n",
    "    x['input_ids'] = tokenizer.batch_encode_plus(\n",
    "            x['text'], \n",
    "            add_special_tokens = True,\n",
    "            max_length = 32,\n",
    "            pad_to_max_length = True,)['input_ids']\n",
    "\n",
    "    return x\n",
    "\n",
    "def _prepare_ds(split):\n",
    "\n",
    "    ds = load_dataset('rotten_tomatoes', split = split)\n",
    "    ds = ds.shuffle(seed = 42)\n",
    "    ds = ds.map(_tokenize, batched  =True)\n",
    "    ds.set_format(type = 'torch', columns = ['input_ids', 'label'])\n",
    "\n",
    "    return ds\n",
    "\n",
    "train_ds, validation_ds, test_ds = map(_prepare_ds, ('train', 'validation', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1,  ..., 1, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
